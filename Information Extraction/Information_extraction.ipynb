{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities Extracted are: [('June 15, 2023', 'DATE'), ('October 5th to October 8th, 2022', 'DATE'), ('September 30, 2022', 'DATE'), ('Yesterday', 'DATE'), ('June 11, 2023', 'DATE'), ('Next week', 'DATE'), ('Friday, June 16th, 2023', 'DATE')]\n",
      "Relations Extracted are : [('I', 'have'), ('conference', 'take'), ('we', 'plan')]\n",
      "Dates Extracted are : ['June 12, 2023', 'June 15, 2023', 'June 12, 2023', 'October 12, 2023', 'June 05, 2023', 'October 12, 2023', 'June 08, 2023', 'June 12, 2022', 'September 12, 2023', 'June 30, 2023', 'June 12, 2022', 'June 12, 2023', 'June 11, 2023', 'June 12, 2023', 'June 16, 2023', 'June 12, 2023', 'June 16, 2023', 'June 12, 2023']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "from dateutil import parser\n",
    "\n",
    "text = \"I have a meeting scheduled on June 15, 2023. \" \\\n",
    "       \"The conference will take place from October 5th to October 8th\" \\\n",
    "       \", 2022. Our project deadline is set for September 30, 2022. \" \\\n",
    "       \"Yesterday's date was June 11, 2023. Next week, \" \\\n",
    "       \"we plan to have a team gathering on Friday, June 16th, 2023.\"\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize sentences into words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words = [[word for word in sentence if word.lower() not in stopwords] for sentence in words]\n",
    "\n",
    "# Load spaCy model and process text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(f\"Named Entities Extracted are: {entities}\")\n",
    "\n",
    "relations = []\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "        subject = token.text\n",
    "        verb = token.head.text\n",
    "        relation = (subject, verb)\n",
    "        relations.append(relation)\n",
    "print(f\"Relations Extracted are : {relations}\")\n",
    "\n",
    "dates = []\n",
    "for word in text.split():\n",
    "    try:\n",
    "        date = parser.parse(word, fuzzy=True)\n",
    "        dates.append(date.strftime(\"%B %d, %Y\"))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "print(f\"Dates Extracted are : {dates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple Inc.', 'ORG'), ('Cupertino', 'GPE'), ('California', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "#Document level relation extraction\n",
    "#Entity Recognition:\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the document\n",
    "doc = nlp(\"Apple Inc. is headquartered in Cupertino, California.\")\n",
    "\n",
    "# Extract named entities\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'This is the second sentence.']\n"
     ]
    }
   ],
   "source": [
    "#sentence boundaries\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Sample document\n",
    "document = \"This is the first sentence. This is the second sentence.\"\n",
    "\n",
    "# Tokenize the document into sentences\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "print(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m sent_resolved \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m sent:\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtoken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoref_clusters\u001B[49m:\n\u001B[0;32m     13\u001B[0m         main_mention \u001B[38;5;241m=\u001B[39m token\u001B[38;5;241m.\u001B[39m_\u001B[38;5;241m.\u001B[39mcoref_clusters[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmain\u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m     14\u001B[0m         sent_resolved\u001B[38;5;241m.\u001B[39mappend(main_mention)\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\spacy\\tokens\\underscore.py:47\u001B[0m, in \u001B[0;36mUnderscore.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extensions:\n\u001B[1;32m---> 47\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE046\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n\u001B[0;32m     48\u001B[0m     default, method, getter, setter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extensions[name]\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m getter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: [E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "#coreference Resolution\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add neuralcoref to the pipeline\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# Now you should be able to use coreference resolution\n",
    "doc = nlp(\"John is a software engineer. He works at a tech company.\")\n",
    "print(doc._.coref_clusters)  # Access the coreference clusters\n",
    "\n",
    "resolved_text = []\n",
    "for sent in doc.sents:\n",
    "    sent_resolved = []\n",
    "    for token in sent:\n",
    "        if token._.coref_clusters:\n",
    "            main_mention = token._.coref_clusters[0].main.text\n",
    "            sent_resolved.append(main_mention)\n",
    "        else:\n",
    "            sent_resolved.append(token.text)\n",
    "    resolved_text.append(\" \".join(sent_resolved))\n",
    "\n",
    "print(\"\\n\".join(resolved_text))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: []\n",
      "Relation: headquartered in\n"
     ]
    }
   ],
   "source": [
    "#Relation Extraction\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Apple Inc., headquartered in Cupertino, California, \" \\\n",
    "    \"was founded by Steve Jobs and Steve Wozniak.Jone is employed there! HE likes to eat pizza.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Define the relation patterns using SpaCy's rule-based Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the patterns for the relation \"headquartered in\"\n",
    "patterns = [\n",
    "    [{'LOWER': 'headquartered'}, {'LOWER': 'in'}],\n",
    "    [{'LOWER': 'founded'}, {'POS': 'ADP'}, {'ENT_TYPE': 'PERSON'}]\n",
    "]\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('Headquartered', patterns ,on_match= None)\n",
    "\n",
    "# Find matches in the document\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Extract the entities and relation\n",
    "entities = []\n",
    "relation = None\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    if relation is None:\n",
    "        relation = doc[start:end].text\n",
    "    for ent in doc[start:end].ents:\n",
    "        entities.append(ent.text)\n",
    "\n",
    "# Print the entities and relation\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Relation:\", relation)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 13\u001B[0m\n\u001B[0;32m      8\u001B[0m document \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124mBarack Obama was born on August 4, 1961, in Honolulu, Hawaii. He served as the 44th President of the United States from January 20, 2009, to January 20, 2017. Obama is a member of the Democratic Party. Michelle Obama, Barack Obama\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms wife, was actively involved in promoting healthy eating and reducing childhood obesity through her \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms Move!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m campaign. She also focused on supporting military families and education initiatives. The Obama family has two daughters, Malia Ann and Natasha Marian, commonly known as Sasha. They have been active in various social and philanthropic endeavors. Barack Obama and Joe Biden worked closely together during their time in office, focusing on issues such as economic recovery, climate change, and foreign policy.\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Perform relation extraction using OpenAI's GPT-3 model\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext-davinci-003\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocument\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mExtract relations:\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\n\u001B[0;32m     20\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Parse the API response to extract relations\u001B[39;00m\n\u001B[0;32m     23\u001B[0m relations \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001B[0m, in \u001B[0;36mCompletion.create\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    137\u001B[0m ):\n\u001B[0;32m    138\u001B[0m     (\n\u001B[0;32m    139\u001B[0m         deployment_id,\n\u001B[0;32m    140\u001B[0m         engine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\openai\\api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    279\u001B[0m     method,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[0;32m    289\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m    290\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    296\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[0;32m    297\u001B[0m     )\n\u001B[1;32m--> 298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\openai\\api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[0;32m    694\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    695\u001B[0m         )\n\u001B[0;32m    696\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[0;32m    697\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    707\u001B[0m     )\n",
      "File \u001B[1;32mc:\\users\\dell\\pycharmprojects\\nlp\\venv\\lib\\site-packages\\openai\\api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    761\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[1;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[0;32m    764\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[0;32m    765\u001B[0m     )\n\u001B[0;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[1;31mRateLimitError\u001B[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentance: Apple Inc. was founded by Steve Jobs.\n",
      "Predicted lable data : ['co-founded']\n",
      "Predicted Relation: co-founded\n"
     ]
    }
   ],
   "source": [
    "#Relation Extraction\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example training data\n",
    "data = [\n",
    "    ('Barack Obama is the President of the United States.', 'Barack Obama', 'President'),\n",
    "    ('Steve Jobs co-founded Apple Inc.', 'Steve Jobs', 'co-founded'),\n",
    "    ('The Eiffel Tower is located in Paris.', 'Eiffel Tower', 'located in'),\n",
    "    ('The book was written by J.K. Rowling.', 'book', 'written by'),\n",
    "    # Add more training examples with different relations and entities\n",
    "]\n",
    "\n",
    "# Extract features from training data using SpaCy\n",
    "def extract_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    features = []\n",
    "    for token in doc:\n",
    "        features.append(token.text)\n",
    "        features.append(token.pos_)\n",
    "        features.append(token.dep_)\n",
    "    return ' '.join(features)\n",
    "\n",
    "# Prepare training data\n",
    "sentences = []\n",
    "entities = []\n",
    "labels = []\n",
    "for sentence, entity, label in data:\n",
    "    sentences.append(extract_features(sentence))\n",
    "    entities.append(entity)\n",
    "    labels.append(label)\n",
    "\n",
    "# Vectorize training data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Train a classifier (e.g., Linear SVM)\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X, labels)\n",
    "\n",
    "# Example test sentence\n",
    "test_sentence = \"Apple Inc. was founded by Steve Jobs.\"\n",
    "\n",
    "# Extract features from test sentence\n",
    "test_features = extract_features(test_sentence)\n",
    "\n",
    "# Vectorize test data\n",
    "test_data = vectorizer.transform([test_features])\n",
    "\n",
    "# Predict the relation class\n",
    "predicted_label = classifier.predict(test_data)\n",
    "\n",
    "# Print the predicted relation class\n",
    "print(f\"Test Sentance: {test_sentence}\")\n",
    "print(f\"Predicted lable data : {predicted_label}\")\n",
    "print(\"Predicted Relation:\", predicted_label[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}